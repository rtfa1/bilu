---
name: researcher
description: Consume a technical research brief and perform deep research (prior art, libraries, standards, architectures), producing a source-backed comparison and recommendation.
metadata:
  short-description: Deep technical research from a brief
---

# Researcher

## Goal

Given a technical idea or a “Research Agent Brief”, perform deep technical research on what exists, what patterns are common, and what the best-fit options are. Produce a structured, source-backed report that another engineer/agent can act on.

## Inputs

Prefer one of:
- A brief generated by the `research-brief` skill (recommended).
- A link to a brief file in-repo (e.g., `docs/briefs/<slug>.md`).
- Raw bullets from the user (fallback).

If no brief is provided, ask for it (or generate a minimal one by asking up to 7 questions).

## Operating Principles

- Be technical and evidence-driven; avoid business/market analysis unless explicitly requested.
- Use primary sources first: official docs/specs, upstream GitHub repos, maintainer statements, RFCs.
- Be explicit about uncertainty: mark claims as **Verified** (with source) or **Unverified** (needs confirmation).
- Prefer “good enough to decide” over exhaustive encyclopedias; timebox research unless the user asks otherwise.

## Network + Sandbox Reality

If network access is restricted, shell web fetches (e.g., `curl`) may require user approval. If you cannot fetch sources:
- Produce a best-effort report from local context and general knowledge.
- Clearly label it as **Unverified** and list the exact URLs/queries to validate once network is allowed.

## Workflow

### 1) Parse the brief into a research plan

Extract:
- Hard constraints (platform, language, licensing, data sensitivity)
- P0 questions (must answer to pick an approach)
- P1/P2 questions (nice-to-have)
- Search seeds (keywords/queries)
- Expected outputs (tables, shortlist, recommendation, PoC suggestions)

### 2) Collect sources (timeboxed)

For each P0 question, gather 3–8 sources across:
- Official documentation/specs
- Upstream repos and issue trackers
- High-signal writeups (engineering blogs, conference talks, papers)

Record per source:
- URL, title, date accessed
- What claim it supports

### 3) Build the option set

Create a shortlist of realistic options (usually 3–7) and compare them against criteria derived from the brief:
- Compatibility (platform/runtime)
- Maturity/maintenance (release cadence, open issues, bus factor)
- License and ecosystem fit
- Security posture (auth model, sandboxing, dependency risks)
- Performance and scalability characteristics
- DX/ergonomics (APIs, docs, examples)

### 4) Recommend an approach

Pick a default recommendation and 1 fallback:
- Explain why in 5–10 bullets tied to the criteria.
- Call out key risks and how to mitigate them.
- List “unknowns that still matter” and the fastest validations (PoCs/tests).

### 5) Produce the report

Output a single Markdown report using `assets/template.md`.

Only create files if the user asks; otherwise paste the report in chat.

## Output Quality Checklist

- Answers every P0 question or states why it could not be answered.
- Includes a comparison table with sources.
- Separates **facts** (with citations/links) from **recommendations**.
- Ends with concrete next actions and minimal PoCs to de-risk.

## Files

Only create files if the user asks. If asked, propose:
- `docs/research/<slug>.md`

## Examples

- “Use `researcher` on `docs/briefs/coffee-online-store-research-brief.md` and recommend a stack + libraries.”
- “Use `researcher` (timebox 30 min): compare auth approaches for a Next.js app with multi-tenant orgs.”
- “Use `researcher`: find existing open-source projects similar to this brief and extract patterns.”

